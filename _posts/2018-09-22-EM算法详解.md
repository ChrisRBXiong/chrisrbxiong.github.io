---
layout:     post
title:      EM算法详解
subtitle:   详解EM算法推导及证明过程，以及应用实例
date:       2018-09-22
author:     chris
header-img: img/em-bg2.png
catalog: true
tags:
    - Bayesian
    - machine learning
---



# 前言

EM（Expectation Maximum）算法，即期望最大化算法，被广泛应用于学习含有隐变量的模型，例如高斯混合模型（GMM）、PLSI、概率主成分分析（PPCA）等。

# 为什么需要EM算法

假设模型可观测变量为$X$，隐变量为$Z$，参数为$\Theta$，则根据极大似然法，我们需要找到参数$\Theta^{*}$，使得模型对数似然$LL$ 取得最大值，$LL$定义如下：
$$
\begin{align}
LL = \ln p(X|\Theta) = \ln \left\{ \int p(X,Z|\Theta)dZ \right\}
\end{align}
$$
由定义可以看到，为了获得最大似然解，需要在对数函数内部对隐含变量$Z$进行积分，而在计算上这往往是不可行的，因此我们需要一种方法来绕过这一困难，这种方法便是EM算法。

# EM算法推导

EM算法通过引入分布$q(z)$来分解对数似然函数：
$$
\begin{align}
	\begin{split}
	\ln p(X|\Theta) &= \int q(Z) \ln p(X|\Theta)dZ \\
	&= \int q(Z) \ln \frac {p(X,Z|\Theta)} {p(Z|X,\Theta)}dZ \\
	&= \int q(Z) \ln \frac {p(X,Z|\Theta)q(Z)} {p(Z|X,\Theta)q(Z)}dZ \\
	&= \int q(Z) \ln \frac {p(X, Z| \Theta)} {q(Z)}dZ + \int q(Z) \ln \frac {q(Z)} {p(Z|X,\Theta)}dZ \\
	&= \underbrace{\int q(Z) \ln p(X, Z| \Theta)dZ - \int q(Z) \ln q(Z) dZ}_{\text ELBO(q, \Theta)} + \underbrace{\int q(Z) \ln \frac {q(Z)}{p(Z|X,\Theta)}dZ}_{\text KL(q(Z)||p(Z|X, \Theta))} \\
	\end{split}
\end{align}
$$
通过式(2)可以看到，$\ln p(X|\Theta)$ 被分解为$ELBO(q, \Theta)$和$KL(q(Z)||p(Z|X, \Theta))$的和，并通过调整$q(Z)$和$\Theta$来使得对数似然$\ln p(X|\Theta)$达到最大值，即：

$$
\begin{align}
	\begin{split}
		q^*,\Theta^* &= \underset{q \in \mathcal{Q}, \Theta} {argmax} \ln p(X | \Theta) \\
		&= \underset{q \in \mathcal{Q}, \Theta} {argmax} \left\{ L(q, \Theta) + KL(q(Z) || p(Z|X, \Theta)) \right\}
	\end{split}
\end{align}
$$
由于$KL(q(Z) || p(Z|X, \Theta)) >=0$，可以得到$L(q, \Theta)$为对数似然$\ln p(X|\Theta)$的下界，即：

$$
\begin{align}
	\ln p(X|\Theta) = L(q, \Theta) + KL(q(Z) || p(Z|X, \Theta))  &>= L(q, \Theta)
\end{align}
$$


通过这种方式，我们从最大化对数似然$\ln p(X|\Theta)$转变为关于$q$和$\Theta$最大化对数似然的变分下界$L(q, \Theta)$，由于
$$
\begin{align}
	KL(q(Z) || p(Z|X, \Theta)) = 0 \iff q(Z) = p(Z|X, \Theta)
\end{align}
$$
且$\ln p(X|\Theta)​$不关于$q​$变化，因此当$KL(q(Z) || p(Z|X, \Theta))​$关于$q​$取得最小值时，相对应的，变分下界$L(q, \Theta)​$关于$q​$取得最大值，这便成为EM算法中的E步骤。
接下来则是关于$\Theta​$优化变分下界$L(q, \Theta)​$，即EM算法的M步，通过将$p(Z|X, \Theta^{旧})​$代入$L(q, \Theta)​$，推导如下：

$$
\begin{align}
	\begin{split}
		L(q, \Theta) &= L(p(Z|X, \Theta^{旧}), \Theta) \\
		&= \int p(Z|X, \Theta^{旧}) \ln p(X, Z| \Theta)dZ - \int p(Z|X, \Theta^{旧}) \ln p(Z|X, \Theta^{旧}) dZ \\
		&= \mathbb{E}_{p(Z|X, \Theta^{旧})}\ln p(X, Z| \Theta) + \mathbb{H}(Z|X, \Theta^{旧}) \\
		&= \mathbb{E}_{p(Z|X, \Theta^{旧})}\ln p(X, Z| \Theta) + 常数 \\
			\underset{\Theta} {max} L(p(&Z|X, \Theta^旧),\Theta) \iff \underset{\Theta} {max} \mathbb{E}_{p(Z|X, \Theta^{旧})}\ln p(X, Z| \Theta)
	\end{split}
\end{align}
$$
如公式（）所示，当关于$q$最大化后，关于$\Theta$最大化变分下界即最大化后验概率期望$\mathbb{E}_{p(Z|X, \Theta^{旧})}\ln p(X, Z| \Theta)$，这也即是EM算法中的M步骤。





