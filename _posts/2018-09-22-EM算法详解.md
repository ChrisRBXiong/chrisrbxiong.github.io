---
layout:     post
title:      EM算法详解
subtitle:   详解EM算法推导及证明过程，以及应用实例
date:       2018-09-22
author:     chris
header-img: img/em-bg2.png
catalog: true
tags:
    - Bayesian
    - machine learning
    - EM
---



# 前言

EM（Expectation Maximum）算法，即期望最大化算法，被广泛应用于学习含有隐变量的模型，例如高斯混合模型（GMM）、PLSI、概率主成分分析（PPCA）等。

# 为什么需要EM算法

假设模型可观测变量为$X$，隐变量为$Z$，参数为$\Theta$，则根据极大似然法，我们需要找到参数$\Theta^{*}$，使得模型对数似然$LL$ 取得最大值，$LL$定义如下：
$$
\begin{align}
LL = \ln p(X|\Theta) = \ln \left\{ \int p(X,Z|\Theta)dZ \right\}
\end{align}
$$
由定义可以看到，为了获得最大似然解，需要在对数函数内部对潜在变量$Z$进行积分，而在计算上这往往是不可行的，因此我们需要一种方法来绕过这一困难，这种方法便是EM算法。

# EM算法推导

EM算法通过引入分布$q(z)$来分解对数似然函数：<br/>
$$
\begin{align}
	\begin{split}
	\ln p(X|\Theta) &= \int q(Z) \ln p(X|\Theta)dZ \\
	&= \int q(Z) \ln \frac {p(X,Z|\Theta)} {p(Z|X,\Theta)}dZ \\
	&= \int q(Z) \ln \frac {p(X,Z|\Theta)q(Z)} {p(Z|X,\Theta)q(Z)}dZ \\
	&= \int q(Z) \ln \frac {p(X, Z| \Theta)} {q(Z)}dZ + \int q(Z) \ln \frac {q(Z)} {p(Z|X,\Theta)}dZ \\
	&= \underbrace{\int q(Z) \ln p(X, Z| \Theta)dZ - \int q(Z) \ln q(Z) dZ}_{\text ELBO(q, \Theta)} + \underbrace{\int q(Z) \ln \frac {q(Z)}{p(Z|X,\Theta)}dZ}_{\text KL(q(Z)||p(Z|X, \Theta))} \\
	\end{split}
\end{align}
$$
<br/>通过式(2)可以看到，$\ln p(X|\Theta)$ 被分解为$ELBO(q, \Theta)$和$KL(q(Z)||p(Z|X, \Theta))$的和，并通过调整$q(Z)$和$\Theta$来使得对数似然$\ln p(X|\Theta)$达到最大值，即：
<br/>$$
\begin{align}
	\begin{split}
		q^*,\Theta^* &= \underset{q \in \mathcal{Q}, \Theta} {argmax} \ln p(X | \Theta) \\
		&= \underset{q \in \mathcal{Q}, \Theta} {argmax} \left\{ L(q, \Theta) + KL(q(Z) || p(Z|X, \Theta)) \right\}
	\end{split}
\end{align}
$$<br/>
由于$KL(q(Z) || p(Z|X, \Theta)) >=0$，可以得到$L(q, \Theta)$为对数似然$\ln p(X|\Theta)$的下界，即：<br/>
$$
\begin{align}
	\ln p(X|\Theta) = L(q, \Theta) + KL(q(Z) || p(Z|X, \Theta))  &>= L(q, \Theta)
\end{align}
$$<br/>
通过这种方式，我们从最大化对数似然$\ln p(X|\Theta)$转变为关于$q$和$\Theta$最大化对数似然的变分下界$L(q, \Theta)$，即$L(q, \Theta)$为概率分布$q(Z)$的泛函，并且为参数$\Theta$的函数。由于<br/>
$$
\begin{align}
	KL(q(Z) || p(Z|X, \Theta)) = 0 \iff q(Z) = p(Z|X, \Theta)
\end{align}
$$<br/>
且$\ln p(X|\Theta)​$不关于$q​$变化，因此当$KL(q(Z) || p(Z|X, \Theta))​$关于$q​$取得最小值时，相对应的，变分下界$L(q, \Theta)​$关于$q​$取得最大值，这便成为EM算法中的E步骤。

接下来则是关于$\Theta​$优化变分下界$L(q, \Theta)​$，即EM算法的M步，通过将$p(Z|X, \Theta^{旧})​$代入$L(q, \Theta)​$，推导如下：<br/>
$$
\begin{align}
	\begin{split}
		L(q, \Theta) &= L(p(Z|X, \Theta^{旧}), \Theta) \\
		&= \int p(Z|X, \Theta^{旧}) \ln p(X, Z| \Theta)dZ - \int p(Z|X, \Theta^{旧}) \ln p(Z|X, \Theta^{旧}) dZ \\
		&= \mathbb{E}_{p(Z|X, \Theta^{旧})}\ln p(X, Z| \Theta) + \mathbb{H}(Z|X, \Theta^{旧}) \\
		&= \mathbb{E}_{p(Z|X, \Theta^{旧})}\ln p(X, Z| \Theta) + 常数 \\
			\underset{\Theta} {max} L(p(&Z|X, \Theta^旧),\Theta) \iff \underset{\Theta} {max} \mathbb{E}_{p(Z|X, \Theta^{旧})}\ln p(X, Z| \Theta)
	\end{split}
\end{align}
$$<br/>
如公式（）所示，当关于$q$最大化后，关于$\Theta$最大化变分下界即最大化后验概率期望$\mathbb{E}_{p(Z|X, \Theta^{旧})}\ln p(X, Z| \Theta)$，即$\mathcal{Q}(\Theta, \Theta^{旧})$，这也即是EM算法中的M步骤。

#### EM算法基本框架
---------------
1. 随机初始化参数$\Theta_0$
2. **For** t = 0,...max_iter-1 **do**
	1. E步骤计算$p(Z\|X,\Theta^t)$
	2. 计算$ELBO(p(Z\|X,\Theta^t),\Theta^t)$，若$ELBO$增加值小于阈值则退出
	3. M步骤 关于$\Theta$最大化$\mathbb{E}_{p(Z\|X,\Theta^{t})}\ln p(X,Z\|\Theta)$，得到$\Theta^{t+1}$

一般而言，E步骤计算得到后验概率用于M步骤的求解，但也存在E步骤计算其他统计量用于M步骤的情况，如PPCA中，E步骤计算$\mathbb{E}[z_n]$和$\mathbb{E}[z_nz_n^T]$用于后续计算过程。

# EM算法收敛性证明
EM算法通过E步骤和M步骤，交替优化$q$和$\Theta$来实现对对数似然函数的最大化。假设当前的参数为$\Theta^{旧}$，在E步骤当中，通过令$q(Z)=p(Z|X, \Theta^{旧})$来最大化泛函$L(q,\Theta)$，该步骤的作用是建立了对数似然函数的下界，对对数似然值并不产生影响。接下来，在M步骤当中，通过对$\Theta$的优化，最大化函数$L(q, \Theta)$，增大对数似然值。假设在M步骤后，获得的最优参数为$\Theta^{新}$。
EM迭代前和EM迭代后的对数似然值如下所示：
$$
\begin{align}
	\begin{split}
	EM迭代前：\ln p(x|\Theta^{旧}) = \mathcal{Q}(\Theta^{旧}, \Theta^{旧}) - \mathbb{H}({\Theta^{旧}, \Theta^{旧}}) + KL(p(Z|X, \Theta^{旧}), p(Z|X, \Theta^{旧})) \\
	EM迭代后：\ln p(x|\Theta^{新}) = \mathcal{Q}(\Theta^{新}, \Theta^{旧}) - \mathbb{H}({\Theta^{新}, \Theta^{旧}}) + KL(p(Z|X, \Theta^{旧}), p(Z|X, \Theta^{新}))
	\end{split}
\end{align}
$$
显而易见，其中的KL项保持了增长，这是因为在对$\Theta$进行调整后，新的后验概率不再与之前计算的$q$等同。由于$\Theta^{新}$使得$\mathcal{Q}(\Theta, \Theta^{旧})$最大化，因此该项也同样获得了增长。
下面证明熵的减少：
$$
\begin{align}
	\begin{split}
	\mathbb{H}({\Theta^{新}, \Theta^{旧}}) - \mathbb{H}({\Theta^{旧}, \Theta^{新}}) &= \int p(Z|X, \Theta^{旧}) \ln p(Z|X, \Theta^{旧}) dZ - \int p(Z|X, \Theta^{旧}) \ln p(Z|X, \Theta^{旧}) dZ \\
	&= -KL(p(Z|X, \Theta^{新}),p(Z|X, \Theta^{旧})) < 0
	\end{split}
\end{align}
$$
由证明可知，在经过EM迭代后，熵项减少，结合另外几项的结论可以证明，每一轮的EM迭代，都会增大对数似然函数，且对数似然函数的增大值大于变分下界的增大值，其差值为KL项的增长。

# 展望
EM算法将最大化似然函数分成了两个较易实现的阶段，即E步骤和M步骤。尽管如此，仍然存在复杂的模型，无法计算E步骤或M步骤，这衍生除出了许多的推广算法，如GEM算法等。

总体而言，EM算法需要解析的计算潜在变量的后验分布，而在很多情况下这是不可行的，针对这一问题，在一定程度上可以采用近似方法来解决。近似方法包括确定近似和随机近似方法。确定近似方法基于对后验概率分布的解析近似来拟合分布，如变分推断法。而随机近似方法，如蒙特卡罗方法，通过采样的方式来估计分布。对比这二种方法，确定近似方法往往具有较高的运算效率，但需要精确推导；而随机近似方法使用简单，但需要大量的运算资源。接下来的章节会对变分推断和蒙特卡罗方法进行介绍。

# EM算法实例
#### 离散潜在变量
- [GMM（高斯混合模型）](/2018/09/23/EM算法之GMM)
- [PLSI（概率潜在语义分析）](/2018/09/23/EM算法之PLSI)

#### 连续潜在变量
- [PPCA（概率主成分分析）](/2018/09/23/EM算法之PPCA)




